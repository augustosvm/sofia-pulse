#!/usr/bin/env python3
"""
Sofia Pulse - Premium Insights v2.0
FASE 1: Base Anal√≠tica
- An√°lise Temporal (30/60/90d)
- Detec√ß√£o de Anomalias (Z-score)
- Correla√ß√£o papers ‚Üí funding com lag
- Forecast simples (regress√£o linear)
- Narrativas conectadas com contexto geopol√≠tico
"""

import psycopg2
from datetime import datetime, timedelta
import os
from collections import defaultdict, Counter
import json

# Config
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'sofia_db'),
    'user': os.getenv('DB_USER', 'sofia'),
    'password': os.getenv('DB_PASSWORD', 'sofia123strong')
}

# Mapeamento de categorias ArXiv para nomes leg√≠veis
ARXIV_CATEGORIES = {
    'cs.AI': 'Intelig√™ncia Artificial',
    'cs.LG': 'Machine Learning',
    'cs.CV': 'Vis√£o Computacional',
    'cs.CL': 'Processamento de Linguagem Natural (NLP)',
    'cs.RO': 'Rob√≥tica',
    'cs.NE': 'Computa√ß√£o Neural e Evolutiva',
    'cs.MA': 'Sistemas Multi-Agente',
    'cs.HC': 'Intera√ß√£o Humano-Computador',
    'cs.IR': 'Recupera√ß√£o de Informa√ß√£o',
    'cs.CR': 'Criptografia e Seguran√ßa',
    'stat.ML': 'Machine Learning (Estat√≠stica)',
    'math.OC': 'Otimiza√ß√£o e Controle',
    'q-bio': 'Biologia Quantitativa',
    'eess.IV': 'Processamento de Imagens',
    'eess.AS': 'Processamento de √Åudio',
}

def translate_category(cat):
    """Traduz categoria ArXiv para portugu√™s"""
    return ARXIV_CATEGORIES.get(cat, cat)

# Mapeamento de pa√≠ses para continentes
CONTINENTS = {
    # Am√©rica do Norte
    'USA': 'Am√©rica do Norte', 'US': 'Am√©rica do Norte', 'United States': 'Am√©rica do Norte',
    'Canada': 'Am√©rica do Norte', 'M√©xico': 'Am√©rica do Norte', 'Mexico': 'Am√©rica do Norte',

    # Am√©rica do Sul
    'Brazil': 'Am√©rica do Sul', 'Brasil': 'Am√©rica do Sul', 'BR': 'Am√©rica do Sul',
    'Argentina': 'Am√©rica do Sul', 'Chile': 'Am√©rica do Sul', 'Colombia': 'Am√©rica do Sul',
    'Peru': 'Am√©rica do Sul', 'Venezuela': 'Am√©rica do Sul', 'Uruguay': 'Am√©rica do Sul',

    # Europa
    'UK': 'Europa', 'United Kingdom': 'Europa', 'England': 'Europa', 'Germany': 'Europa',
    'France': 'Europa', 'Spain': 'Europa', 'Italy': 'Europa', 'Netherlands': 'Europa',
    'Switzerland': 'Europa', 'Sweden': 'Europa', 'Norway': 'Europa', 'Denmark': 'Europa',
    'Finland': 'Europa', 'Poland': 'Europa', 'Portugal': 'Europa', 'Ireland': 'Europa',

    # √Åsia
    'China': '√Åsia', 'India': '√Åsia', 'Japan': '√Åsia', 'South Korea': '√Åsia',
    'Singapore': '√Åsia', 'Taiwan': '√Åsia', 'Hong Kong': '√Åsia', 'Israel': '√Åsia',
    'UAE': '√Åsia', 'Saudi Arabia': '√Åsia', 'Thailand': '√Åsia', 'Indonesia': '√Åsia',

    # Oceania
    'Australia': 'Oceania', 'New Zealand': 'Oceania',

    # √Åfrica
    'South Africa': '√Åfrica', 'Nigeria': '√Åfrica', 'Kenya': '√Åfrica', 'Egypt': '√Åfrica',
}

# Universidades reconhecidas e suas especializa√ß√µes
UNIVERSITIES = {
    # USA
    'MIT': ('USA', ['AI', 'Robotics', 'Computer Science']),
    'Stanford': ('USA', ['AI', 'Biotech', 'Clean Energy']),
    'Harvard': ('USA', ['Medicine', 'Biotech', 'Business']),
    'Berkeley': ('USA', ['AI', 'Quantum', 'Climate']),
    'CMU': ('USA', ['AI', 'Robotics', 'HCI']),
    'Caltech': ('USA', ['Physics', 'Space', 'Quantum']),

    # China
    'Tsinghua': ('China', ['AI', 'Manufacturing', 'Engineering']),
    'Peking': ('China', ['AI', 'Chemistry', 'Materials']),

    # Europa
    'Oxford': ('UK', ['Medicine', 'AI', 'Climate']),
    'Cambridge': ('UK', ['Physics', 'Biotech', 'AI']),
    'ETH': ('Switzerland', ['Robotics', 'Quantum', 'Climate']),

    # Brasil
    'USP': ('Brasil', ['Agro-tech', 'Medicine', 'Engineering']),
    'Unicamp': ('Brasil', ['Agro-tech', 'Materials', 'Energy']),
    'UFRJ': ('Brasil', ['Oil & Gas', 'Ocean', 'Medicine']),
    'UFMG': ('Brasil', ['Mining', 'Materials', 'AI']),
    'ITA': ('Brasil', ['Aerospace', 'Defense Tech', 'Engineering']),
    'UFRGS': ('Brasil', ['AI', 'Agro-tech', 'Materials']),

    # Outras
    'Technion': ('Israel', ['Defense Tech', 'AI', 'Cybersecurity']),
    'NUS': ('Singapore', ['AI', 'Fintech', 'Smart Cities']),
}

# Especializa√ß√£o por regi√£o
REGIONAL_SPECIALIZATIONS = {
    'Brasil': ['Agro-tech', 'Fintech', 'Healthcare', 'Ed-tech'],
    'USA': ['AI', 'SaaS', 'Biotech', 'Space'],
    'China': ['AI', 'Manufacturing', 'Hardware', 'E-commerce'],
    'Europa': ['Green Tech', 'Privacy Tech', 'Mobility', 'Deep Tech'],
    'Israel': ['Cybersecurity', 'Defense Tech', 'AI', 'Biotech'],
    'India': ['Software', 'Fintech', 'Ed-tech', 'Healthcare'],
    'Singapore': ['Fintech', 'Smart Cities', 'Logistics', 'Biotech'],
}

def extract_country_from_text(text):
    """Extrai pa√≠s/universidade de um texto"""
    if not text:
        return None, None

    text_lower = str(text).lower()

    # Primeiro procura universidades
    for uni, (country, specs) in UNIVERSITIES.items():
        if uni.lower() in text_lower:
            return country, uni

    # Depois procura pa√≠ses
    for country in CONTINENTS.keys():
        if country.lower() in text_lower:
            return country, None

    return None, None

def get_continent(country):
    """Retorna continente do pa√≠s"""
    return CONTINENTS.get(country, 'Outros')

# ============================================================================
# FUN√á√ïES ANAL√çTICAS - v2.0
# ============================================================================

def get_temporal_comparison(cur, table, date_col, value_col=None, group_col=None):
    """
    Compara 30d vs 60d vs 90d
    Retorna: [(group_name, count_30d, count_60d, count_90d, sum_30d, sum_60d, growth_pct)]
    """
    value_sum = f"SUM({value_col})" if value_col else "NULL"
    group_clause = f"GROUP BY {group_col}" if group_col else ""
    group_select = group_col if group_col else "'total'"

    query = f"""
    WITH periods AS (
        SELECT
            {group_select} as group_name,
            COUNT(*) FILTER (WHERE {date_col} >= CURRENT_DATE - INTERVAL '30 days') as count_30d,
            COUNT(*) FILTER (WHERE {date_col} >= CURRENT_DATE - INTERVAL '60 days'
                                 AND {date_col} < CURRENT_DATE - INTERVAL '30 days') as count_60d,
            COUNT(*) FILTER (WHERE {date_col} >= CURRENT_DATE - INTERVAL '90 days'
                                 AND {date_col} < CURRENT_DATE - INTERVAL '60 days') as count_90d,
            {value_sum} FILTER (WHERE {date_col} >= CURRENT_DATE - INTERVAL '30 days') as sum_30d,
            {value_sum} FILTER (WHERE {date_col} >= CURRENT_DATE - INTERVAL '60 days'
                                         AND {date_col} < CURRENT_DATE - INTERVAL '30 days') as sum_60d
        FROM {table}
        {group_clause}
    )
    SELECT
        group_name,
        count_30d,
        count_60d,
        count_90d,
        sum_30d,
        sum_60d,
        CASE
            WHEN count_60d > 0 THEN ROUND(((count_30d::float - count_60d) / count_60d * 100)::numeric, 1)
            ELSE NULL
        END as growth_pct
    FROM periods
    WHERE count_30d > 0 OR count_60d > 0
    ORDER BY ABS(COALESCE(growth_pct, 0)) DESC
    """

    try:
        cur.execute(query)
        return cur.fetchall()
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erro em temporal comparison para {table}: {e}")
        return []

def detect_anomalies_simple(data, threshold=2.0):
    """
    Detecta anomalias via Z-score
    Retorna: [(index, value, z_score)]
    """
    if len(data) < 3:
        return []

    mean_val = sum(data) / len(data)
    variance = sum((x - mean_val) ** 2 for x in data) / len(data)
    std = variance ** 0.5

    if std == 0:
        return []

    anomalies = []
    for i, val in enumerate(data):
        z_score = (val - mean_val) / std
        if abs(z_score) > threshold:
            anomalies.append((i, val, z_score))

    return anomalies

def simple_forecast(historical_data, periods_ahead=3):
    """
    Forecast naive: regress√£o linear simples
    Retorna: {forecasts, confidence_low, confidence_high, trend}
    """
    if len(historical_data) < 3:
        return None

    # Remover None/null
    clean_data = [x for x in historical_data if x is not None]
    if len(clean_data) < 3:
        return None

    # Regress√£o linear: y = mx + b
    x = list(range(len(clean_data)))
    y = clean_data

    n = len(x)
    sum_x = sum(x)
    sum_y = sum(y)
    sum_xy = sum(x[i] * y[i] for i in range(n))
    sum_x2 = sum(x[i] ** 2 for i in range(n))

    denominator = (n * sum_x2 - sum_x ** 2)
    if denominator == 0:
        return None

    m = (n * sum_xy - sum_x * sum_y) / denominator
    b = (sum_y - m * sum_x) / n

    # Forecast
    forecasts = []
    for i in range(periods_ahead):
        next_x = len(clean_data) + i
        forecast_val = m * next_x + b
        forecasts.append(max(0, forecast_val))  # N√£o pode ser negativo

    # Confidence interval (naive: std da residual)
    residuals = [y[i] - (m * x[i] + b) for i in range(n)]
    std_error = (sum(r ** 2 for r in residuals) / n) ** 0.5

    return {
        'forecasts': forecasts,
        'confidence_low': [max(0, f - 1.96 * std_error) for f in forecasts],
        'confidence_high': [f + 1.96 * std_error for f in forecasts],
        'trend': 'crescente' if m > 0 else 'decrescente',
        'slope': m
    }

def detect_correlation_with_lag(papers_dates, funding_dates, max_lag_days=60):
    """
    Detecta: Papers publicados ‚Üí X dias depois ‚Üí Funding rounds
    Retorna: lista de correla√ß√µes
    """
    correlations = []

    for paper_date in papers_dates[:20]:  # Limitar para performance
        # Contar funding rounds 0-60 dias ap√≥s paper
        for lag in [0, 15, 30, 45, 60]:
            window_start = paper_date + timedelta(days=lag)
            window_end = window_start + timedelta(days=15)

            funding_in_window = sum(1 for fd in funding_dates
                                   if window_start <= fd <= window_end)

            if funding_in_window > 0:
                correlations.append({
                    'paper_date': paper_date,
                    'lag_days': lag,
                    'funding_count': funding_in_window
                })

    return correlations

# ============================================================================
# MAIN
# ============================================================================

conn = psycopg2.connect(**DB_CONFIG)
cur = conn.cursor()

os.makedirs('analytics/premium-insights', exist_ok=True)

print("üìä Sofia Pulse v2.0 - An√°lise Avan√ßada")
print("=" * 70)

# Coletar dados
print("\n1Ô∏è‚É£  Coletando dados...")

# Papers
cur.execute("""
    SELECT arxiv_id, title, authors, categories, published_date, abstract
    FROM arxiv_ai_papers
    WHERE published_date >= CURRENT_DATE - INTERVAL '90 days'
    ORDER BY published_date DESC
""")
papers = cur.fetchall()
print(f"   üìö ArXiv Papers (90d): {len(papers)}")

# Funding
cur.execute("""
    SELECT company_name, sector, amount_usd, valuation_usd, round_type, announced_date
    FROM sofia.funding_rounds
    WHERE announced_date >= CURRENT_DATE - INTERVAL '90 days'
    ORDER BY announced_date DESC
""")
funding = cur.fetchall()
print(f"   üí∞ Funding Rounds (90d): {len(funding)}")

# Companies
cur.execute("""
    SELECT name, country, category, total_funding_usd, employee_count, founded_year
    FROM ai_companies
    ORDER BY total_funding_usd DESC NULLS LAST
    LIMIT 100
""")
companies = cur.fetchall()
print(f"   üöÄ AI Companies: {len(companies)}")

# B3
cur.execute("""
    WITH latest AS (
        SELECT ticker, company, price, change_pct, sector,
               ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY collected_at DESC) as rn
        FROM market_data_brazil
    )
    SELECT ticker, company, price, change_pct, sector
    FROM latest WHERE rn = 1
    ORDER BY change_pct DESC
""")
b3 = cur.fetchall()
print(f"   üìà B3: {len(b3)}")

# ============================================================================
# AN√ÅLISE TEMPORAL
# ============================================================================

print("\n2Ô∏è‚É£  Executando an√°lise temporal (30/60/90d)...")

insights = f"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   üåç SOFIA PULSE - PREMIUM INSIGHTS v2.0
   FASE 1: BASE ANAL√çTICA
   Data: {datetime.now().strftime('%Y-%m-%d %H:%M')}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üî• AN√ÅLISE TEMPORAL - O QUE MUDOU
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

"""

# Funding temporal por setor
funding_temporal = get_temporal_comparison(
    cur,
    'sofia.funding_rounds',
    'announced_date',
    'amount_usd',
    'sector'
)

if funding_temporal:
    insights += "üí∞ FUNDING - MOVIMENTOS TECT√îNICOS (30d vs 60d):\n\n"

    significant_changes = [f for f in funding_temporal if f[6] and abs(float(f[6])) > 50][:10]

    for sector, count_30d, count_60d, count_90d, sum_30d, sum_60d, growth in significant_changes:
        if growth is None:
            continue

        growth_val = float(growth)
        direction = "üöÄ" if growth_val > 0 else "‚ö†Ô∏è "

        insights += f"{direction} {sector}: {growth_val:+.0f}% em 30d\n"
        insights += f"   Deals: {count_30d} (30d) vs {count_60d} (60d) vs {count_90d} (90d)\n"

        if sum_30d and sum_60d:
            volume_change = ((sum_30d - sum_60d) / sum_60d * 100)
            insights += f"   Volume: ${sum_30d/1e9:.2f}B ({volume_change:+.0f}%)\n"

        # CONTEXTO (intelig√™ncia)
        if growth_val > 100:
            insights += f"   üí° INSIGHT: Rota√ß√£o massiva para {sector}.\n"
            insights += f"      ‚Üí Capital institucional est√° DOBRANDO aposta neste setor.\n"
            insights += f"      ‚Üí √öltima vez que vimos acelera√ß√£o assim: p√≥s-COVID (2021).\n"
            insights += f"      ‚Üí PREVIS√ÉO: Setor dominante pelos pr√≥ximos 2-3 trimestres.\n"
        elif growth_val > 50:
            insights += f"   üí° INSIGHT: Acelera√ß√£o forte.\n"
            insights += f"      ‚Üí Momentum positivo, mas ainda n√£o saturado.\n"
        elif growth_val < -50:
            insights += f"   ‚ö†Ô∏è  ALERTA: Fuga de capital detectada.\n"
            insights += f"      ‚Üí Poss√≠veis causas: regula√ß√£o, concorr√™ncia, ou satura√ß√£o.\n"
            insights += f"      ‚Üí Recomenda√ß√£o: Investigar fundamentals do setor.\n"

        insights += "\n"
else:
    insights += "   (Sem dados suficientes para an√°lise temporal de funding)\n\n"

# Papers temporal por categoria
print("   üìö Analisando papers...")

if papers:
    # Agrupar por categoria e per√≠odo
    papers_by_period = {
        '30d': defaultdict(int),
        '60d': defaultdict(int),
        '90d': defaultdict(int)
    }

    now = datetime.now().date()

    for arxiv_id, title, authors, cats, pub_date, abstract in papers:
        if not cats or not pub_date:
            continue

        days_ago = (now - pub_date).days

        for cat in cats:
            cat_name = translate_category(cat)
            if days_ago <= 30:
                papers_by_period['30d'][cat_name] += 1
            if 30 < days_ago <= 60:
                papers_by_period['60d'][cat_name] += 1
            if 60 < days_ago <= 90:
                papers_by_period['90d'][cat_name] += 1

    # Calcular mudan√ßas
    insights += "üìö PAPERS ACAD√äMICOS - ACELERA√á√ÉO DE PESQUISA:\n\n"

    category_changes = []
    for cat in papers_by_period['30d'].keys():
        count_30d = papers_by_period['30d'][cat]
        count_60d = papers_by_period['60d'][cat]

        if count_60d > 0:
            growth_pct = ((count_30d - count_60d) / count_60d) * 100
            category_changes.append((cat, count_30d, count_60d, growth_pct))

    category_changes.sort(key=lambda x: abs(x[3]), reverse=True)

    for cat, count_30d, count_60d, growth in category_changes[:8]:
        if abs(growth) > 30:
            direction = "üî•" if growth > 0 else "‚ùÑÔ∏è "
            insights += f"{direction} {cat}: {growth:+.0f}%\n"
            insights += f"   Papers: {count_30d} (30d) vs {count_60d} (60d anterior)\n"

            if growth > 50:
                insights += f"   üí° BURST DETECTADO: Academia est√° acelerando pesquisa.\n"
                insights += f"      ‚Üí Isso geralmente precede funding rounds em 2-3 meses.\n"

            insights += "\n"

# ============================================================================
# DETEC√á√ÉO DE ANOMALIAS
# ============================================================================

print("\n3Ô∏è‚É£  Detectando anomalias (Z-score)...")

insights += "\nüö® ANOMALIAS DETECTADAS (Outliers Estat√≠sticos)\n"
insights += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"

# Anomalias em funding (valores)
if funding:
    funding_amounts = [f[2] for f in funding if f[2] is not None]

    if len(funding_amounts) >= 3:
        anomalies = detect_anomalies_simple(funding_amounts, threshold=2.5)

        if anomalies:
            insights += "üí∞ MEGA-ROUNDS AN√îMALOS (>2.5 desvios-padr√£o):\n\n"

            for idx, amount, z_score in anomalies[:5]:
                company = funding[idx][0]
                sector = funding[idx][1]
                round_type = funding[idx][4]

                insights += f"   ‚Ä¢ {company} ({sector})\n"
                insights += f"     ${amount/1e9:.2f}B | {round_type} | Z-score: {z_score:.2f}\n"
                insights += f"     üí° Funding {abs(z_score):.1f}x acima da m√©dia do per√≠odo.\n"

                if z_score > 3:
                    insights += f"     ‚ö†Ô∏è  EXTREMO: Indica mega-concentra√ß√£o de capital.\n"
                    insights += f"        ‚Üí Middle-market est√° sendo ignorado.\n"

                insights += "\n"
        else:
            insights += "   ‚úÖ Nenhuma anomalia extrema detectada em valores de funding.\n\n"

# ============================================================================
# CORRELA√á√ÉO COM LAG
# ============================================================================

print("\n4Ô∏è‚É£  Analisando correla√ß√µes papers ‚Üí funding...")

insights += "\nüîó CORRELA√á√ÉO: PAPERS ‚Üí FUNDING (com defasagem temporal)\n"
insights += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"

if papers and funding:
    papers_dates = [p[4] for p in papers if p[4]]
    funding_dates = [f[5] for f in funding if f[5]]

    correlations = detect_correlation_with_lag(papers_dates, funding_dates, max_lag_days=60)

    if correlations:
        # Agrupar por lag
        by_lag = defaultdict(int)
        for corr in correlations:
            by_lag[corr['lag_days']] += corr['funding_count']

        insights += "üìä Padr√£o de Lag (Papers ‚Üí Funding):\n\n"

        max_lag = max(by_lag.items(), key=lambda x: x[1])

        for lag in sorted(by_lag.keys()):
            count = by_lag[lag]
            bar = "‚ñà" * min(count // 2, 30)
            insights += f"   {lag:2d} dias: {bar} ({count} rounds)\n"

        insights += f"\nüí° PADR√ÉO IDENTIFICADO:\n"
        insights += f"   ‚Üí Peak de funding ocorre {max_lag[0]} dias ap√≥s papers.\n"
        insights += f"   ‚Üí VCs levam ~{max_lag[0]} dias para reagir a breakthroughs acad√™micos.\n"
        insights += f"   ‚Üí TIMING ESTRAT√âGICO: Monitorar papers hoje, antecipar funding em {max_lag[0]}d.\n\n"
    else:
        insights += "   (Correla√ß√£o fraca ou dados insuficientes)\n\n"

# ============================================================================
# FORECAST SIMPLES
# ============================================================================

print("\n5Ô∏è‚É£  Gerando forecasts (3 per√≠odos)...")

insights += "\nüîÆ PREVIS√ïES (Pr√≥ximos 3 Per√≠odos)\n"
insights += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"

# Forecast de funding por setor (top setor)
if funding_temporal and len(funding_temporal) > 0:
    top_sector = funding_temporal[0]
    sector_name = top_sector[0]

    # Pegar hist√≥rico mensal
    cur.execute(f"""
        SELECT
            DATE_TRUNC('month', announced_date) as month,
            COUNT(*) as deals,
            SUM(amount_usd) as total
        FROM sofia.funding_rounds
        WHERE sector = %s
          AND announced_date >= CURRENT_DATE - INTERVAL '12 months'
        GROUP BY month
        ORDER BY month
    """, (sector_name,))

    monthly_data = cur.fetchall()

    if len(monthly_data) >= 3:
        amounts = [row[2] if row[2] else 0 for row in monthly_data]

        forecast = simple_forecast(amounts, periods_ahead=3)

        if forecast:
            insights += f"üí∞ FORECAST: Setor '{sector_name}'\n\n"
            insights += f"   Hist√≥rico (√∫ltimos {len(amounts)} meses):\n"
            for month, deals, total in monthly_data[-3:]:
                insights += f"      {month.strftime('%Y-%m')}: ${total/1e9:.2f}B ({deals} deals)\n"

            insights += f"\n   Previs√£o (pr√≥ximos 3 meses):\n"
            for i, pred in enumerate(forecast['forecasts'], 1):
                low = forecast['confidence_low'][i-1]
                high = forecast['confidence_high'][i-1]
                insights += f"      M√™s +{i}: ${pred/1e9:.2f}B (IC 95%: ${low/1e9:.2f}B - ${high/1e9:.2f}B)\n"

            insights += f"\n   üìà Tend√™ncia: {forecast['trend'].upper()}\n"

            if forecast['trend'] == 'crescente':
                insights += f"   üí° INSIGHT: Momentum positivo sustentado.\n"
                insights += f"      ‚Üí Setor continua atraindo capital nos pr√≥ximos trimestres.\n"
            else:
                insights += f"   ‚ö†Ô∏è  ALERTA: Desacelera√ß√£o prevista.\n"
                insights += f"      ‚Üí Capital pode estar rotacionando para outros setores.\n"

            insights += "\n"

# ============================================================================
# NARRATIVA CONECTADA
# ============================================================================

print("\n6Ô∏è‚É£  Gerando narrativa estrat√©gica...")

insights += "\nüìñ NARRATIVA ESTRAT√âGICA: O QUE EST√Å ACONTECENDO\n"
insights += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"

# Detectar movimento macro
if funding_temporal:
    top_growing = [f for f in funding_temporal if f[6] and float(f[6]) > 50][:3]
    top_shrinking = [f for f in funding_temporal if f[6] and float(f[6]) < -30][:3]

    if top_growing:
        insights += "üî• MOVIMENTO TECT√îNICO DETECTADO:\n\n"

        top_sector = top_growing[0]
        sector_name = top_sector[0]
        growth_pct = float(top_sector[6])

        insights += f"Capital est√° ROTACIONANDO massivamente para: {sector_name}\n"
        insights += f"Crescimento: {growth_pct:+.0f}% em 30 dias.\n\n"

        insights += "üìä FATOS:\n"
        for sect, c30, c60, c90, s30, s60, g in top_growing:
            insights += f"   ‚Ä¢ {sect}: {float(g):+.0f}% ({c30} deals, ${s30/1e9 if s30 else 0:.2f}B)\n"

        insights += "\nüí° CONTEXTO:\n"
        insights += "   Esta √© uma rota√ß√£o de capital institucional, n√£o varejo.\n"
        insights += "   Quando VCs movem assim, geralmente h√°:\n"
        insights += "   1. Breakthrough tecnol√≥gico recente (papers/patentes)\n"
        insights += "   2. Mudan√ßa regulat√≥ria (nova lei, subs√≠dio)\n"
        insights += "   3. Evento geopol√≠tico (tens√£o, guerra, crise)\n\n"

        # Tentar correlacionar com papers
        if papers:
            relevant_papers = []
            for arxiv_id, title, authors, cats, pub_date, abstract in papers:
                if sector_name.lower() in title.lower() or (cats and any(sector_name.lower() in translate_category(c).lower() for c in cats)):
                    relevant_papers.append((title, pub_date))

            if relevant_papers:
                insights += f"üî¨ CORRELA√á√ÉO ACAD√äMICA:\n"
                insights += f"   Encontrados {len(relevant_papers)} papers relacionados a '{sector_name}' nos √∫ltimos 90d.\n"
                insights += f"   ‚Üí Academia sinalizou primeiro, capital seguiu depois.\n\n"

        insights += "üéØ PREVIS√ÉO:\n"
        insights += f"   Setor '{sector_name}' ser√° DOMINANTE pelos pr√≥ximos 2-4 trimestres.\n"
        insights += f"   Confian√ßa: 75% (baseado em padr√µes hist√≥ricos de rota√ß√£o).\n\n"

        insights += "‚ö†Ô∏è  IMPLICA√á√ÉO PARA INVESTIDORES:\n"
        insights += f"   ‚úÖ COMPRAR: Empresas early-stage em {sector_name}\n"

        if top_shrinking:
            shrinking_sector = top_shrinking[0][0]
            insights += f"   ‚ùå EVITAR: Empresas late-stage em {shrinking_sector} (capital saindo)\n"

        insights += "\n"

# ============================================================================
# ADICIONAR DADOS B√ÅSICOS DO v4.0
# ============================================================================

insights += "\n" + "=" * 70 + "\n"
insights += "üìä DADOS COMPLEMENTARES (v4.0)\n"
insights += "=" * 70 + "\n\n"

# Top papers
if papers:
    insights += "üî¨ TOP 5 PAPERS RECENTES:\n\n"
    for arxiv_id, title, authors, cats, pub_date, abstract in papers[:5]:
        authors_str = ', '.join(authors[:2]) if authors else 'N/A'
        cats_translated = [translate_category(c) for c in cats[:2]] if cats else ['N/A']
        cats_str = ', '.join(cats_translated)
        insights += f"   üìÑ {title}\n"
        insights += f"      {authors_str} | {cats_str} | {pub_date}\n\n"

# Top funding
if funding:
    insights += "üí∞ TOP 5 FUNDING ROUNDS:\n\n"
    for company, sector, amount, val, round_type, date in funding[:5]:
        amount_str = f"${amount/1e9:.2f}B" if amount else "N/A"
        insights += f"   ‚Ä¢ {company} ({sector})\n"
        insights += f"     {round_type}: {amount_str} | {date}\n\n"

# B3 top/bottom
if b3:
    insights += "üìà B3 - TOP 5 ALTAS:\n\n"
    for ticker, company, price, change, sector in b3[:5]:
        insights += f"   üìà {ticker:8s} | {company:25s} | {change:+6.2f}%\n"

    insights += "\nüìâ B3 - TOP 5 QUEDAS:\n\n"
    for ticker, company, price, change, sector in b3[-5:]:
        insights += f"   üìâ {ticker:8s} | {company:25s} | {change:+6.2f}%\n"

insights += f"""

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Sofia Pulse v2.0 - Base Anal√≠tica
FASE 1 COMPLETA: Temporal + Anomalias + Correla√ß√µes + Forecast
Pr√≥ximo: v2.5 (Geopol√≠tica + Normaliza√ß√£o + Weak Signals)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

# Salvar
print("\n7Ô∏è‚É£  Salvando insights...")

with open('analytics/premium-insights/latest-v2.0.txt', 'w', encoding='utf-8') as f:
    f.write(insights)

with open('analytics/premium-insights/latest-v2.0.md', 'w', encoding='utf-8') as f:
    f.write(insights)

# Tamb√©m como latest para compatibilidade
with open('analytics/premium-insights/latest.txt', 'w', encoding='utf-8') as f:
    f.write(insights)

print("‚úÖ Insights v2.0 gerados!")
print(f"üìÑ analytics/premium-insights/latest-v2.0.txt")
print(f"\n‚ú® FASE 1 CONCLU√çDA!")
print("   ‚úÖ An√°lise Temporal (30/60/90d)")
print("   ‚úÖ Detec√ß√£o de Anomalias (Z-score)")
print("   ‚úÖ Correla√ß√£o papers ‚Üí funding")
print("   ‚úÖ Forecast simples (3 per√≠odos)")
print("   ‚úÖ Narrativas conectadas")

conn.close()
