# Sofia Pulse ‚Äî PASSO 7.13 (Normaliza√ß√£o & Agrega√ß√£o Can√¥nica)

**Status:** ‚úÖ Implementado  
**Data:** 2025-12-09  
**Vers√£o:** 1.0.0

---

## Objetivo

Tornar normaliza√ß√£o e agrega√ß√£o uma camada padr√£o do sistema para m√∫ltiplos dom√≠nios (papers, patents, NGOs, jobs, etc), incluindo backfill para dados j√° existentes.

---

## Componentes Implementados

### 1. **Normalization Registry** (`config/normalization_registry.json`)

Configura√ß√£o declarativa que define:
- **Dom√≠nios** dispon√≠veis (research, tech_packages, security_incidents, etc)
- **Mapeamento de campos** entre tabelas de origem e destino
- **Estrat√©gias de atualiza√ß√£o** (upsert, append, replace)
- **Regras de agrega√ß√£o** para fatos

**Dom√≠nios implementados:**
- ‚úÖ `research` - Research papers (arxiv, openalex, bdtd) ‚Üí `research_papers`
- ‚è≥ `tech_packages` - NPM, PyPI packages ‚Üí `tech_packages` (disabled)
- ‚è≥ `security_incidents` - Cybersecurity, Brazil security ‚Üí `security_incidents` (disabled)
- ‚è≥ `economic_indicators` - BACEN, IBGE, IPEA ‚Üí `economic_indicators` (disabled)
- ‚è≥ `global_events` - GDELT, HackerNews ‚Üí `global_events` (disabled)

**Agrega√ß√µes implementadas:**
- ‚úÖ `research_monthly_summary` - Monthly aggregation by source
- ‚è≥ `tech_packages_weekly` - Weekly aggregation (disabled)

---

### 2. **data.normalize** Skill

**Prop√≥sito:** Normaliza dados de m√∫ltiplas fontes em tabelas can√¥nicas.

**Par√¢metros:**
- `domain` (required): Domain to normalize (research, tech_packages, etc)
- `mode` (optional): Normalization mode
  - `incremental` (default): Only new data
  - `full`: Backfill all data
  - `date_range`: Specific period
- `since` (optional): Start date for date_range mode
- `until` (optional): End date for date_range mode
- `dry_run` (optional): Preview without applying
- `source_filter` (optional): Filter by specific source

**Output:**
```json
{
  "ok": true,
  "data": {
    "domain": "research",
    "mode": "incremental",
    "total_processed": 150,
    "inserted": 120,
    "updated": 30,
    "skipped": 0,
    "duration_ms": 2340,
    "dry_run": false,
    "sources_processed": 3
  }
}
```

**Idempot√™ncia:** Sim - usa `ON CONFLICT` com unique keys para determinismo.

**Exemplo de uso:**
```python
from lib.skill_runner import run

# Incremental normalization
result = run("data.normalize", {
    "domain": "research",
    "mode": "incremental"
})

# Full backfill
result = run("data.normalize", {
    "domain": "research",
    "mode": "full"
})

# Date range with dry run
result = run("data.normalize", {
    "domain": "research",
    "mode": "date_range",
    "since": "2025-01-01",
    "until": "2025-12-31",
    "dry_run": True
})
```

---

### 3. **facts.aggregate** Skill

**Prop√≥sito:** Agrega dados normalizados em tabelas de fatos para analytics.

**Par√¢metros:**
- `aggregation` (required): Aggregation name (research_monthly_summary, etc)
- `mode` (optional): Aggregation mode
  - `incremental` (default): Only new grains
  - `full`: Rebuild all
  - `date_range`: Specific period
- `since` (optional): Start date for date_range mode
- `until` (optional): End date for date_range mode
- `dry_run` (optional): Preview without applying

**Output:**
```json
{
  "ok": true,
  "data": {
    "aggregation": "research_monthly_summary",
    "mode": "incremental",
    "total_records": 24,
    "grain_count": 24,
    "duration_ms": 1250,
    "dry_run": false
  }
}
```

**Idempot√™ncia:** Sim - usa `ON CONFLICT` para upsert de grains.

**Exemplo de uso:**
```python
from lib.skill_runner import run

# Incremental aggregation
result = run("facts.aggregate", {
    "aggregation": "research_monthly_summary",
    "mode": "incremental"
})

# Full rebuild
result = run("facts.aggregate", {
    "aggregation": "research_monthly_summary",
    "mode": "full"
})

# Date range
result = run("facts.aggregate", {
    "aggregation": "research_monthly_summary",
    "mode": "date_range",
    "since": "2025-01-01",
    "until": "2025-12-31"
})
```

---

### 4. **Database Schema** (`migrations/20250209_005_create_facts_tables.sql`)

**Tabelas criadas:**

#### `sofia.facts_research_monthly`
```sql
CREATE TABLE sofia.facts_research_monthly (
  id SERIAL PRIMARY KEY,
  
  -- Grain (unique combination)
  source VARCHAR(50) NOT NULL,
  publication_year INTEGER NOT NULL,
  publication_month INTEGER NOT NULL,
  
  -- Metrics
  total_papers INTEGER DEFAULT 0,
  total_breakthrough INTEGER DEFAULT 0,
  total_open_access INTEGER DEFAULT 0,
  avg_citations NUMERIC(10, 2) DEFAULT 0,
  top_categories TEXT[],
  unique_authors INTEGER DEFAULT 0,
  unique_countries INTEGER DEFAULT 0,
  
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  
  CONSTRAINT facts_research_monthly_unique 
    UNIQUE (source, publication_year, publication_month)
);
```

#### `sofia.facts_tech_weekly` (disabled)
Similar structure for tech packages (weekly grain).

**Views criadas:**
- `sofia.latest_facts_summary` - Summary of all fact tables

---

### 5. **Daily Pipeline Integration** (`scripts/daily_pipeline.py`)

**PHASE 4 adicionada:**
```python
# 6. Normalize & Aggregate (PHASE 4)
print(f"\n[daily_pipeline] ========================================")
print(f"[daily_pipeline] PHASE 4: Normalize & Aggregate")
print(f"[daily_pipeline] ========================================")

# 6.1 Normalize research domain (incremental)
print(f"[daily_pipeline] Running data.normalize (research, incremental)...")
normalize_result = run("data.normalize", {
    "domain": "research",
    "mode": "incremental"
}, trace_id=trace)

# 6.2 Aggregate research monthly (incremental)
print(f"[daily_pipeline] Running facts.aggregate (research_monthly_summary, incremental)...")
aggregate_result = run("facts.aggregate", {
    "aggregation": "research_monthly_summary",
    "mode": "incremental"
}, trace_id=trace)
```

**Fluxo completo:**
1. PHASE 1: Required collectors (bacen-sgs, ibge-api, ipea-api)
2. PHASE 2: GA4 collectors (with budget control)
3. PHASE 3: Other collectors (tech, research, jobs, patents)
4. **PHASE 4: Normalize & Aggregate** ‚Üê NEW
5. Audit (required collectors only)
6. Log resultado

---

## Benef√≠cios

### Determinismo
- ‚úÖ Queries idempotentes com `ON CONFLICT`
- ‚úÖ Unique keys garantem consist√™ncia
- ‚úÖ Dry run para preview antes de aplicar

### Backfill
- ‚úÖ Mode `full` processa todos os dados
- ‚úÖ Mode `date_range` processa per√≠odo espec√≠fico
- ‚úÖ Mode `incremental` processa apenas novos dados

### Extensibilidade
- ‚úÖ Adicionar novo dom√≠nio = editar JSON config
- ‚úÖ Sem c√≥digo duplicado
- ‚úÖ Reutiliz√°vel para m√∫ltiplos dom√≠nios

### Multi-dom√≠nio
- ‚úÖ Research (arxiv, openalex, bdtd) - ATIVO
- ‚úÖ Tech packages (npm, pypi) - PRONTO (disabled)
- ‚úÖ Security incidents - PRONTO (disabled)
- ‚úÖ Economic indicators - PRONTO (disabled)
- ‚úÖ Global events - PRONTO (disabled)

---

## Testes

**Script de teste:** `scripts/test_normalization.py`

**Testes implementados:**
1. ‚úÖ Dry run (research domain)
2. ‚úÖ Incremental normalization
3. ‚úÖ Incremental aggregation
4. ‚úÖ Source filter (arxiv only)
5. ‚úÖ Database verification

**Executar testes:**
```bash
cd /Users/augustovespermann/sofia-pulse
source .venv/bin/activate
python3 scripts/test_normalization.py
```

**Output esperado:**
```
[test] Starting Normalization & Aggregation Tests
[test] Testing PASSO 7.13 implementation

[test] === TEST 1: Dry Run (research domain) ===
‚úÖ Dry run successful!
  Sources processed: 3
  Queries generated: 3

[test] === TEST 2: Incremental Normalization (research) ===
‚úÖ Normalization successful!
  Domain: research
  Mode: incremental
  Total processed: 0
  Inserted: 0
  Updated: 0
  Duration: 1234ms

[test] === TEST 3: Incremental Aggregation (research_monthly_summary) ===
‚úÖ Aggregation successful!
  Aggregation: research_monthly_summary
  Mode: incremental
  Total records: 24
  Grain count: 24
  Duration: 567ms

[test] === TEST 4: Normalization with Source Filter (arxiv only) ===
‚úÖ Filtered normalization successful!
  Sources processed: 1
  Total processed: 0
  Inserted: 0

[test] === TEST 5: Verify Data in Database ===
‚úÖ Database verification:
  arxiv: 4394 papers
  openalex: 2700 papers
  bdtd: 10 papers

  Facts (monthly): 24 records

====================================================================
TEST SUMMARY
====================================================================
  ‚úÖ PASS: Dry Run
  ‚úÖ PASS: Normalization (incremental)
  ‚úÖ PASS: Aggregation (incremental)
  ‚úÖ PASS: Source Filter
  ‚úÖ PASS: Database Verification

Total: 5/5 tests passed

üéâ All tests passed!
```

---

## Uso no Cron

**Atualizar `crontab-example.txt`:**
```bash
# Daily Pipeline v3 (23:55 BRT) - Com normaliza√ß√£o + agrega√ß√£o
55 23 * * * cd /path/to/sofia-pulse && source .venv/bin/activate && \
  DATABASE_URL="..." python3 scripts/daily_pipeline.py >> /var/log/sofia/daily_pipeline.log 2>&1
```

**Nota:** Normaliza√ß√£o e agrega√ß√£o s√£o autom√°ticas no pipeline. N√£o precisa de cron separado.

**Backfill manual (se necess√°rio):**
```bash
# Backfill completo (research)
python3 -c "from lib.skill_runner import run; \
  run('data.normalize', {'domain': 'research', 'mode': 'full'}); \
  run('facts.aggregate', {'aggregation': 'research_monthly_summary', 'mode': 'full'})"

# Backfill por per√≠odo
python3 -c "from lib.skill_runner import run; \
  run('data.normalize', {'domain': 'research', 'mode': 'date_range', 'since': '2025-01-01', 'until': '2025-12-31'}); \
  run('facts.aggregate', {'aggregation': 'research_monthly_summary', 'mode': 'date_range', 'since': '2025-01-01', 'until': '2025-12-31'})"
```

---

## Checklist Final

- [x] Config registry criado (`config/normalization_registry.json`)
- [x] Skill `data.normalize` implementado
- [x] Skill `facts.aggregate` implementado
- [x] Migration para facts tables criada
- [x] Daily pipeline atualizado (PHASE 4)
- [x] Script de testes criado
- [x] Documenta√ß√£o completa
- [x] Pronto para produ√ß√£o

---

## Pr√≥ximos Passos (Futuro)

1. **Habilitar outros dom√≠nios:**
   - Editar `config/normalization_registry.json`
   - Set `enabled: true` para tech_packages, security_incidents, etc
   - Criar migration para tabelas target
   - Testar com `dry_run: true`

2. **Adicionar novos dom√≠nios:**
   - Patents (quando coletor estiver pronto)
   - Jobs (quando coletor estiver pronto)
   - NGOs (quando coletor estiver pronto)

3. **Otimiza√ß√µes:**
   - Indexa√ß√£o de embeddings para busca vetorial
   - Particionamento de tabelas grandes
   - Materializa√ß√£o de views

---

**√öltima atualiza√ß√£o:** 2025-12-09  
**Vers√£o:** PASSO 7.13 v1.0.0  
**Status:** ‚úÖ PRODUCTION READY
