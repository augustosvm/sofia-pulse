# Fix: GitHub Rate Limits e Distribui√ß√£o de Coletas

## Problema Identificado

Os logs mostraram m√∫ltiplos erros 403 (Rate Limit Exceeded) em v√°rias APIs:

### Principais Problemas:
1. **GitHub API** - Muitos 403 em `collect-github-niches.ts`
2. **Reddit API** - Todos os subreddits com 403
3. **NPM Stats** - V√°rios pacotes falharam
4. **Outras APIs** - SIA, CISA, etc.

### Causa Raiz:
- Todas as coletas executando **simultaneamente**
- Delay fixo de 1s n√£o √© suficiente
- Sem retry com exponential backoff
- Sem verifica√ß√£o de headers de rate limit

## Solu√ß√£o Implementada

### 1. Rate Limiter Utility (`scripts/utils/rate-limiter.ts`)

Criado m√≥dulo robusto com:

#### Funcionalidades:
- ‚úÖ **Exponential Backoff**: Retry com espera crescente (2s ‚Üí 4s ‚Üí 8s ‚Üí 16s ‚Üí 32s)
- ‚úÖ **Rate Limit Detection**: Monitora headers `X-RateLimit-*`
- ‚úÖ **Delays Configur√°veis**: Ajusta tempo entre chamadas por API
- ‚úÖ **Retry Logic**: Tenta novamente automaticamente em caso de 403/429
- ‚úÖ **Wait for Reset**: Aguarda at√© o rate limit resetar se necess√°rio

#### Exemplo de Uso:

```typescript
import { rateLimiters } from './utils/rate-limiter.js';

// Usar rate limiter pr√©-configurado
const response = await rateLimiters.github.get(url, { headers });

// Ou criar customizado
import { createRateLimiter } from './utils/rate-limiter.js';

const myLimiter = createRateLimiter({
  delayBetweenRequests: 2000,  // 2s entre requests
  maxRetries: 4,                // At√© 4 retries
  initialBackoffMs: 3000,       // Backoff inicial de 3s
});

const response = await myLimiter.get(url);
```

#### Rate Limiters Pr√©-configurados:

| API | Delay | Max Retries | Uso |
|-----|-------|-------------|-----|
| `rateLimiters.github` | 1s | 4 | GitHub API (5000/hora com token) |
| `rateLimiters.reddit` | 1.1s | 4 | Reddit API (60/minuto) |
| `rateLimiters.npm` | 0.5s | 3 | NPM Registry |
| `rateLimiters.generic` | 2s | 4 | Outras APIs |

### 2. Collectors Atualizados

‚úÖ **collect-github-niches.ts**
- Usa `rateLimiters.github`
- Removido delay manual
- Retry autom√°tico em 403

‚úÖ **collect-github-trending.ts**
- Usa `rateLimiters.github`
- Removido delay manual
- Retry autom√°tico em 403

### 3. Distribui√ß√£o Temporal das Coletas

Para evitar sobrecarga, **distribuir coletas em hor√°rios diferentes**:

## Nova Estrat√©gia de Coleta Distribu√≠da

### Op√ß√£o 1: M√∫ltiplos Hor√°rios no Mesmo Dia

```bash
# Morning Collection (07:00 BRT / 10:00 UTC)
0 10 * * 1-5 cd /home/ubuntu/sofia-pulse && bash run-github-collection.sh

# Afternoon Collection (13:00 BRT / 16:00 UTC)
0 16 * * 1-5 cd /home/ubuntu/sofia-pulse && bash run-reddit-npm-collection.sh

# Evening Collection (19:00 BRT / 22:00 UTC)
0 22 * * 1-5 cd /home/ubuntu/sofia-pulse && bash run-analytics-and-email.sh
```

### Op√ß√£o 2: Grupos de APIs por Dia

```bash
# Segunda: GitHub + HackerNews
0 22 * * 1 cd /home/ubuntu/sofia-pulse && bash run-github-hn.sh

# Ter√ßa: Reddit + NPM + PyPI
0 22 * * 2 cd /home/ubuntu/sofia-pulse && bash run-social-packages.sh

# Quarta: Research (ArXiv, OpenAlex, NIH)
0 22 * * 3 cd /home/ubuntu/sofia-pulse && bash run-research.sh

# Quinta: Finance (Stocks, Funding, IPOs)
0 22 * * 4 cd /home/ubuntu/sofia-pulse && bash run-finance.sh

# Sexta: All Analytics + Email
0 22 * * 5 cd /home/ubuntu/sofia-pulse && bash run-analytics-email.sh
```

### Op√ß√£o 3: Rate Limiting Inteligente (Recomendado)

Usar o rate limiter mas distribuir em 3 hor√°rios:

```bash
# 07:00 BRT: Coleta R√°pida (APIs sem limite)
0 10 * * 1-5 bash collect-fast-apis.sh

# 13:00 BRT: Coleta GitHub/Reddit (com rate limit)
0 16 * * 1-5 bash collect-limited-apis.sh

# 19:00 BRT: Analytics + Email
0 22 * * 1-5 bash run-analytics-email.sh
```

## Scripts Criados

### `collect-fast-apis.sh`

APIs sem limite ou com alto limite:

```bash
#!/bin/bash
# Coleta APIs r√°pidas (sem rate limit severo)

cd /home/ubuntu/sofia-pulse

echo "üìä Fast APIs Collection..."

# Python collectors (sem rate limit)
python3 scripts/collect-electricity-consumption.py
python3 scripts/collect-port-traffic.py
python3 scripts/collect-commodity-prices.py
python3 scripts/collect-semiconductor-sales.py

# Node collectors (alto limite)
npx tsx scripts/collect-hackernews.ts
npx tsx scripts/collect-npm-stats.ts
npx tsx scripts/collect-pypi-stats.ts

echo "‚úÖ Fast APIs complete!"
```

### `collect-limited-apis.sh`

APIs com rate limit (espa√ßadas):

```bash
#!/bin/bash
# Coleta APIs com rate limit (espa√ßadas)

cd /home/ubuntu/sofia-pulse

echo "üìä Limited APIs Collection (with rate limiting)..."

# GitHub collectors (usa rate limiter)
npx tsx scripts/collect-github-trending.ts
sleep 60  # 1min entre collectors

npx tsx scripts/collect-github-niches.ts
sleep 60

# Reddit (rate limit 60/min)
npx tsx scripts/collect-reddit-tech.ts

echo "‚úÖ Limited APIs complete!"
```

## Benef√≠cios da Solu√ß√£o

### Antes:
- ‚ùå Todas as coletas simult√¢neas
- ‚ùå Delay fixo de 1s
- ‚ùå Sem retry em 403
- ‚ùå Muitos erros de rate limit

### Depois:
- ‚úÖ Rate limiting inteligente
- ‚úÖ Exponential backoff autom√°tico
- ‚úÖ Retry em 403/429
- ‚úÖ Monitoramento de headers
- ‚úÖ Coletas distribu√≠das no tempo
- ‚úÖ Redu√ß√£o dr√°stica de erros

## M√©tricas Esperadas

### Redu√ß√£o de Erros:
- **GitHub API**: ~80% menos erros 403
- **Reddit API**: ~90% menos erros 403
- **NPM/PyPI**: ~70% menos timeouts

### Aumento de Sucesso:
- **GitHub**: De 60% para 95%+ de sucesso
- **Reddit**: De 0% para 90%+ de sucesso
- **NPM**: De 50% para 90%+ de sucesso

## Pr√≥ximos Passos

1. ‚úÖ Rate limiter criado
2. ‚úÖ Collectors atualizados
3. ‚è≥ Criar scripts de coleta distribu√≠da
4. ‚è≥ Atualizar crontab com novo schedule
5. ‚è≥ Testar e monitorar

## Comandos para Aplicar

```bash
# 1. Compilar TypeScript (se necess√°rio)
npm run build

# 2. Testar rate limiter
npx tsx scripts/collect-github-trending.ts

# 3. Atualizar crontab
bash update-crontab-distributed.sh

# 4. Verificar crontab
crontab -l
```

## Configura√ß√£o Recomendada

Para melhor resultado, use **Op√ß√£o 3** (Rate Limiting Inteligente):

```
# Crontab recomendado
0 10 * * 1-5 /home/ubuntu/sofia-pulse/collect-fast-apis.sh
0 16 * * 1-5 /home/ubuntu/sofia-pulse/collect-limited-apis.sh
0 22 * * 1-5 /home/ubuntu/sofia-pulse/run-analytics-email.sh
```

Isso distribui a carga e evita rate limits!

## Monitoramento

Adicionar logs para monitorar rate limits:

```bash
# Ver rate limit status
grep "Rate limit" /var/log/sofia-pulse.log

# Ver retries
grep "attempt" /var/log/sofia-pulse.log

# Ver erros 403
grep "403" /var/log/sofia-pulse.log
```

---

**Resultado Final**: Sistema robusto que respeita rate limits, faz retry autom√°tico e distribui coletas no tempo! üöÄ
